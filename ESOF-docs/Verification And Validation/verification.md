4. Verification and Validation 
===================

1) Degree of Testability of the software program
Topics: Discuss how 'testable' is the program. Discuss how to improve the testability of software components.

The testability of software components (modules, classes) is determined by factors such as:
 
- Controllability - It is impossible to access Components Under Test (CUT) with the given test suite.
 

- Observability - Well explicit. Information about what module is being tested and even a traceback will be displayed in the console with appropriate and noticeable separation. In the end of the test run a compilation of final results (mostly statistical) will also be displayed.
 

- Isolateability: It is possible to run tests on a specific module by adding module name as the last argument when running the test suite. One can also state the directory where the specific tests one wants to run are stored.
 

 
- Separation of concerns: Altough the current test name is displayed it may sometimes not suffice to properly identify what is being tested but mostly the tests have a good separation of concerns given that they are as unitary as possible. As a matter of fact, Django's creators themselves clearly specify in the documentation that:
> Generally tests should be:
>    Unitary (as much as possible). i.e. should test as much as possible only one function/method/class. That’s the very >definition of unit tests. Integration tests are interesting too obviously, but require more time to maintain since they have a >higher probability of breaking.
    
And they seem to follow their own advices.


- Understandability: As stated, altough sometimes test names alone are not enough to accurately identify what component exactly is being tested, it is also displayed which test exactly is being ran so a quick peek on the test code - only in the few cases of need for clarification - should be enough to fully understand the running test.
 


- Heterogeneity: The most recent test suite for Django-cms includes diverse tools such selenium (http://www.seleniumhq.org/), sphinx (http://sphinx-doc.org/), pyenchant(http://pythonhosted.org/pyenchant/) and others as specified in https://github.com/SofiaReis/django-cms/blob/develop/test_requirements/requirements_base.txt but as some tools are purely aesthetic the main testing tool would be Selenium.

##4.1. How is Django-CMS tested?

As we mentioned in some [chapters before](https://github.com/SofiaReis/django-cms/blob/develop/ESOF-docs/Requirements%20elicitation/requirements.md#21-issues-on-django-cms), Django-CMS it's really restrict about tests. To contribute to the plataform, you have to attach tests mandatorily. According Django-CMS, tests should be **unitary**, should test as much as possible only one function or class; **short running** and **easy to understand**. If you are a developer, you use Django-CMS and want to write and run tests, you can see how to do that in [here](http://docs.django-cms.org/en/latest/contributing/testing.html).

To test Django-CMS, the main core developers established some programs to use in the test process that you can consult in [here](https://github.com/divio/django-cms/blob/develop/test_requirements/requirements_base.txt). The most important packages/programs used for testing Django-CMS are:
>- Coverage: It's a python package and measures code average, typically during test execution. Coverage uses the code analysis tools and tracing hooks provided in the Python standard library to determine which lines are executable, and which have been executed. (We use coverage mainly for make report in point 4.3)
-> Coveralls: Service to publish your coverage stats online with a lot of nice features. This package provides seamless integration with coverage.py in your python projects. It makes custom report for data generated by coverage.py package and sends it to json API of coveralls.io service. This package works with any CI (continuous integration) environment. But Django-CMS uses [Travis for CI](https://travis-ci.org/divio/django-cms) to integrate code into a shared repository several times a day. Each check-in is then verified by an automated build, allowing teams to detect problems early. By integrating regularly, you can detect errors quickly, and locate them more easily. 
-> Unittest-xml-reporting: It's a unittest test runner that can save test results to XML files that can be consumed by a wide range of tools, such as build systems, IDEs and continuous integration servers.
-> Selenium: It automates browsers. It's a software testing framework for web applications. Selenium provides a record/playback tool for authoring tests without learning a test scripting language (Selenium IDE). The tests can be run against most modern web browsers.
-> Django-debug-toolbar: A configurable set of panels that display various debug information about the current request/response.
-> Django-better-test: A better test command for Django. Allows you to use --parallel to run tests in parallel (distributed as evenly as possible across your CPU cores) and --isolate to run each test in a separate process to detect test that leak state. You can also quickly re-run the tests failed in the last run using --failed.
-> Pyflakes: A simple program which checks Python source files for errors. Pyflakes analyzes programs and detects various errors. It works by parsing the source file, not importing it, so it is safe to use on modules with side effects. It’s also much faster.






##4.2. How to improve software test?


##4.3. Tests Statistics

We made some studies about the Django-CMS tests statics, like how many unit tests it uses, code coverage, system and performance.

About the unit-tests, we found in the git repository two folders named [test_utils](https://github.com/SofiaReis/django-cms/tree/develop/cms/test_utils) and [tests](https://github.com/SofiaReis/django-cms/tree/develop/cms/tests). 


Now, about coverage... Django-CMS uses Coverage tool
After analysing Django-CMS tests code with the coverage tool available with the python installer, the final results we got describe the test coverage as 28% over the code. We understand that this value doesn't show a big efficency of the tests code due to its low rate gotten before.

     Number of tests (# tests unitários; # tests de sistema, # tests de desempenho, ...)
     % coverage (given by tools like EclEmma)


##4.4. Bug Report
3) [Opcional] Take a bug report, create test cases to reproduce it, and fix it, eventually using automated software fault diagnosis techniques. (grade >18)
